{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from download_tools.plugins.mouselab_mdp import preprocess_mouselab_data, add_click_count_columns\n",
    "from download_tools.plugins.survey_html_form import process_html_demographics\n",
    "from download_tools.plugins.survey_multi_choice import score_mouselab_questionnaires, get_mouselab_quiz_name, get_quiz_passer_ids, score_row, score_generic_questionnaires\n",
    "from download_tools.plugins.survey_text import preprocess_survey_text, get_old_demographics\n",
    "from download_tools.plugins.utils import get_demo_string\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# paths to use\n",
    "inputs_path = Path(data_path).joinpath('inputs')\n",
    "raw_data_path = Path(data_path).joinpath('raw')\n",
    "processed_data_path = Path(data_path).joinpath(f'processed/{analysis_run}')\n",
    "processed_data_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(inputs_path.joinpath(f\"exp_inputs/rewards/{ground_truth_file}.json\")) as json_file:\n",
    "    ground_truths = json.load(json_file)\n",
    "    \n",
    "with open(inputs_path.joinpath(f\"questionnaire_files/questionnaire_OnePart.txt\"), \"rb\") as f:\n",
    "    questionnaire_presentation = json.load(f)\n",
    "reverse_coded_dictionary = {}\n",
    "for quest_info in questionnaire_presentation.values():\n",
    "    for quest in quest_info[\"questions\"]:\n",
    "        reverse_coded_dictionary[quest[\"question_id\"]] = quest[\"reverse_coded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "full_data = {}\n",
    "\n",
    "# read in sessions\n",
    "for run in sessions:\n",
    "    for file_path in raw_data_path.glob(f\"{run}/*.csv\"):\n",
    "        # don't want to save identifiable bonuses\n",
    "        # file, information is already in data\n",
    "        if \"bonuses\" not in str(file_path):\n",
    "            file_name = file_path.stem\n",
    "            print(file_name)\n",
    "            curr_data_frame = pd.read_csv(file_path)\n",
    "            curr_data_frame[\"run\"] = analysis_run\n",
    "\n",
    "            # remove participant who answered they were too young\n",
    "            curr_data_frame = curr_data_frame[~curr_data_frame[\"pid\"].isin(participants_to_remove)]\n",
    "            curr_data_frame = curr_data_frame.rename({\"response\" : \"responses\"}, axis=\"columns\")\n",
    "\n",
    "            if \"internal_node_id\" in curr_data_frame.columns:\n",
    "                curr_data_frame[\"name\"] = curr_data_frame.apply(lambda row : name_mapping[row[\"internal_node_id\"]] if row[\"internal_node_id\"] in name_mapping.keys() else row[\"name\"], axis=1)\n",
    "\n",
    "            if file_name not in full_data:\n",
    "                full_data[file_name] = [curr_data_frame]\n",
    "            else:\n",
    "                full_data[file_name].append(curr_data_frame)\n",
    "\n",
    "full_data = {k: pd.concat(v) for k,v in full_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# people who opened it multiple times, just keep last\n",
    "full_data[\"general_info\"] = full_data[\"general_info\"].drop_duplicates(keep=\"last\", subset=[\"pid\" ,\"run\"])\n",
    "full_data[\"question_data\"] = full_data[\"question_data\"].drop_duplicates(keep=\"last\", subset=[\"pid\" ,\"run\"])\n",
    "\n",
    "individual_variables = full_data[\"general_info\"].merge(full_data[\"question_data\"], on=[\"pid\",\"run\"])\n",
    "\n",
    "begin_hit = full_data[\"survey\"].groupby(\"pid\").min()[\"time_elapsed\"]\n",
    "end_hit = full_data['survey-text'][full_data['survey-text'][\"responses\"].apply(lambda responses: \"Q3\" not in responses)].groupby(\"pid\").max()[\"time_elapsed\"]\n",
    "finished_df = individual_variables[individual_variables[\"pid\"].isin(end_hit.index)].copy(deep=True)\n",
    "finished_df[\"time_diff\"] =  finished_df[\"pid\"].apply(lambda pid: (end_hit[pid])/(60*1000))\n",
    "individual_variables = individual_variables.merge(finished_df[[\"time_diff\", \"pid\", \"run\"]], how=\"left\", on=[\"pid\", \"run\"])\n",
    "\n",
    "# check saved cost makes sense\n",
    "if (COST is not None) and (DEPTH is not None):\n",
    "    if isinstance(COST, dict):\n",
    "        assert(np.all(individual_variables.apply(lambda row: row[\"COST\"] == COST[row[\"codeversion\"]][int(row[\"cond\"])],axis=1)))\n",
    "    else:\n",
    "        unique_costs = np.unique(individual_variables[\"COST\"])\n",
    "        assert(len(unique_costs) == 1)\n",
    "        assert(unique_costs[0] == COST)\n",
    "    if DEPTH:\n",
    "        if isinstance(DEPTH, dict):\n",
    "            assert(np.all(individual_variables.apply(lambda row: row[\"DEPTH\"] == DEPTH[row[\"codeversion\"]][int(row[\"cond\"])],axis=1)))\n",
    "        else:\n",
    "            unique_costs = np.unique(individual_variables[\"DEPTH\"])\n",
    "            assert(len(unique_costs) == 1)\n",
    "            assert(unique_costs[0] == DEPTH)\n",
    "    print(finished_df.groupby([\"DEPTH\", \"COST\"]).mean())\n",
    "else:\n",
    "    print(finished_df.groupby([\"cond\"]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_fields = {\"startTime\":lambda date: np.nan if not isinstance(date,str) else datetime.strptime(date[:15], \"%a %b %d %Y\") ,\"beginhit\":lambda date: np.nan if  not isinstance(date,str) else datetime.fromisoformat(date.split(\" \")[0]), \"beginexp\" : lambda date: np.nan if  not isinstance(date,str) else datetime.fromisoformat(date.split(\" \")[0]), \"endhit\" : lambda date: np.nan if  not isinstance(date,str) else datetime.fromisoformat(date.split(\" \")[0])}\n",
    "\n",
    "for time_field, time_func in time_fields.items():\n",
    "    individual_variables[time_field] = individual_variables[time_field].apply(time_func)\n",
    "\n",
    "individual_variables[\"beginhit\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# delete dates / possible location info in general info\n",
    "del individual_variables[\"startTime\"]\n",
    "del individual_variables[\"beginhit\"]\n",
    "del individual_variables[\"beginexp\"]\n",
    "del individual_variables[\"endhit\"]\n",
    "del individual_variables[\"language\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(inputs_path.joinpath(f\"questionnaire_files/questionnaire_OnePart.txt\"), \"rb\") as f:\n",
    "    questionnaire_presentation = json.load(f)\n",
    "reverse_coded_dictionary = {}\n",
    "for quest_namq, quest_info in questionnaire_presentation.items():\n",
    "    for quest in quest_info[\"questions\"]:\n",
    "        reverse_coded_dictionary[quest[\"question_id\"]] = quest[\"reverse_coded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey = full_data[\"survey\"][full_data[\"survey\"][\"name\"] == \"survey\"].copy(deep=True)\n",
    "\n",
    "\n",
    "survey[\"pages\"] = survey[\"responses\"].apply(lambda responses: [[page_num for question_id, response in page.items()] for page_num, page in eval(responses).items() if page \\\n",
    "                                                              ] )\n",
    "\n",
    "survey[\"question_id\"] = survey[\"responses\"].apply(lambda responses: [[question_id  for question_id, response in page.items()] for page_num, page in eval(responses).items() if page \\\n",
    "                                             ] )\n",
    "\n",
    "survey[\"reverse_coded\"] = survey[\"responses\"].apply(lambda responses: [[reverse_coded_dictionary[question_id]  for question_id, response in page.items()] for page_num, page in eval(responses).items() if page \\\n",
    "                                             ] )\n",
    "survey[\"name\"] = survey[\"responses\"].apply(lambda responses: [[question_id.split(\".\")[0]  for question_id, response in page.items()] for page_num, page in eval(responses).items() if page \\\n",
    "                                             ] )\n",
    "survey[\"responses\"] = survey[\"responses\"].apply(lambda responses: [[ response for question_id, response in page.items()] for page_num, page in eval(responses).items() if page \\\n",
    "                                                                  ] )\n",
    "\n",
    "\n",
    "exploded_survey=survey.explode(column=[\"responses\",\"question_id\", \"reverse_coded\", \"name\", \"pages\"]).explode(column=[\"responses\",\"question_id\", \"reverse_coded\", \"name\", \"pages\"])\n",
    "\n",
    "with open(inputs_path.joinpath(f\"questionnaire_files/solutions_OnePart.pkl\"), \"rb\") as f:\n",
    "    questionnaires_presentation = pickle.load(f)\n",
    "\n",
    "assert len(np.unique(exploded_survey[[\"question_id\", \"reverse_coded\"]].drop_duplicates().groupby([\"question_id\"]).count()[\"reverse_coded\"])) == 1\n",
    "\n",
    "# question_id to 0 or 1 (whether reversed)\n",
    "reversed_questions = dict(exploded_survey[[\"question_id\", \"reverse_coded\"]].to_records(index=False))\n",
    "\n",
    "adjusted_questionnaires_presentation = {\"catch\" : {}}\n",
    "\n",
    "for key, val in questionnaires_presentation.items():\n",
    "    new_key = [k.split(\".\")[0] for k in val.keys()][0]\n",
    "    if new_key not in adjusted_questionnaires_presentation:\n",
    "        adjusted_questionnaires_presentation[new_key] = {}\n",
    "    adjusted_questionnaires_presentation[new_key] = {**adjusted_questionnaires_presentation[new_key], **val}\n",
    "\n",
    "    for item_key, curr_score_dict in adjusted_questionnaires_presentation[new_key].items():\n",
    "        if \"catch\" in item_key:\n",
    "            adjusted_questionnaires_presentation[\"catch\"][item_key] = curr_score_dict\n",
    "        if item_key in reversed_questions and reversed_questions[item_key] == 1:\n",
    "            reverse_score_dict = dict(zip(sorted(curr_score_dict.keys(), reverse=False), [curr_score_dict[k] for k in sorted(curr_score_dict.keys(), reverse=True)]))\n",
    "            adjusted_questionnaires_presentation[new_key][item_key] = reverse_score_dict\n",
    "\n",
    "exploded_survey[\"score\"] = exploded_survey.apply(lambda row: adjusted_questionnaires_presentation[row[\"name\"]][row[\"question_id\"]][row[\"responses\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_responses_per_page = exploded_survey.groupby([\"pid\", \"pages\"], as_index=False).nunique()\\\n",
    "[[\"pid\", \"pages\", \"responses\", \"reverse_coded\", \"question_id\"]].rename(columns={\"question_id\" : \"num_items_page\"})\n",
    "unique_responses_per_page[\"suspicious\"] = unique_responses_per_page.apply(lambda row: row[\"reverse_coded\"]==2 and  row[\"num_items_page\"] > 5 and  row[\"responses\"] == 1, axis=1)\n",
    "\n",
    "straightliners = list(unique_responses_per_page[unique_responses_per_page[\"suspicious\"]][\"pid\"].unique())\n",
    "catch_failures = list(np.unique(exploded_survey[(exploded_survey[\"question_id\"]==\"catch.1\")&(exploded_survey[\"score\"]==0)].pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRT trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_new_crt(crt_df, crt_quiz_solutions):\n",
    "    crt_df[\"name\"] = \"crt\"\n",
    "    crt_df[\"responses\"] = crt_df[\"responses\"].apply(lambda crt_string: crt_string.replace(\"“\", \"\").replace(\"”\", \"\"))\n",
    "\n",
    "    scored_questionnaire_df = score_generic_questionnaires(crt_df, {analysis_run: {\"crt\": crt_quiz_solutions}}, open_ended=True,\n",
    "                                                           group_identifier=\"name\", default_open_ended={\"crt\": \"other\"})\n",
    "    return scored_questionnaire_df\n",
    "\n",
    "crt_quiz_solutions = {\"crt1\": {\".10\": \"intuitive\", \"10\": \"intuitive\", \".05\": \"correct\", \"5\": \"correct\", \"\": \"no response\"}, \"crt2\": {\"100\": \"intuitive\", \"5\": \"correct\", \"\": \"no response\"}, \"crt3\": {\"24\": \"intuitive\", \"47\": \"correct\", \"\": \"no response\"}, \"crt4\": {\"9\": \"intuitive\", \"4\": \"correct\", \"\": \"no response\"}, \"crt5\": {\"30\": \"intuitive\", \"29\": \"correct\", \"\": \"no response\"}, \"crt6\": {\"10\": \"intuitive\", \"20\": \"correct\", \"\": \"no response\"}, \"crt7\": {\"is ahead of where he began\": \"intuitive\", \"has lost money\": \"correct\", \"\": \"no response\"}}\n",
    "crt = full_data[\"survey-html-form\"][full_data[\"survey-html-form\"][\"name\"]==\"crt\"].copy(deep=True)\n",
    "\n",
    "scored_crt = score_new_crt(crt, crt_quiz_solutions)\n",
    "crt_mapping = {\"correct\": 1, \"other\": 0, \"intuitive\":0, \"no response\":0}\n",
    "scored_crt[\"score\"] = scored_crt[\"score\"].apply(lambda score: crt_mapping[score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IQ trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data[\"survey-multi-choice\"][\"name\"] = \"IQ\"\n",
    "\n",
    "scored_iq = score_generic_questionnaires(full_data[\"survey-multi-choice\"], {analysis_run: questionnaires_presentation},group_identifier=\"name\", default_open_ended={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Combined questionnaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_questionnaire_df = pd.concat([exploded_survey, scored_iq, scored_crt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "demographics_df = full_data[\"survey-html-form\"][full_data[\"survey-html-form\"][\"name\"]==\"demographics\"].copy(deep=True)\n",
    "\n",
    "# participant put negative age (confirmed with prolific demographics)\n",
    "demographics_df.loc[demographics_df[\"pid\"]==346 , \"responses\"]=demographics_df.loc[demographics_df[\"pid\"]==346 , \"responses\"].apply(lambda responses: responses.replace('-63','63'))\n",
    "\n",
    "# participants with prolific data only\n",
    "prolific_demographics = pd.read_csv(processed_data_path.joinpath(\"prolific_demographics.csv\"), index_col=0)\n",
    "pids_with_prolific_demographics_only = set(prolific_demographics[\"Participant id\"]) - set(demographics_df[\"pid\"])\n",
    "\n",
    "prolific_demographics[\"Sex\"] = prolific_demographics[\"Sex\"].apply(lambda gender: gender.lower() if gender in [\"Male\", \"Female\"] else \"other\")\n",
    "\n",
    "new_rows = []\n",
    "for row_idx, row in prolific_demographics[\n",
    "    prolific_demographics[\"Participant id\"].isin(pids_with_prolific_demographics_only)].iterrows():\n",
    "    new_rows.append({\"pid\": row[\"Participant id\"], \"responses\" : f\"{{'gender' : '{row['Sex']}', 'age' : '{row['Age']}', 'effort' : '-1'}}\"})\n",
    "\n",
    "demographics_df = pd.concat([demographics_df, pd.DataFrame(new_rows)])\n",
    "demographics, demo_text = process_html_demographics(demographics_df)\n",
    "print(demo_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouselab_datas = preprocess_mouselab_data(full_data[\"mouselab-mdp\"],original_trials_per_block if ranges_to_extract else trials_per_block,ground_truths)\n",
    "\n",
    "mouselab_datas = mouselab_datas.merge(individual_variables, how=\"left\", on=[\"pid\", \"run\"])\n",
    "\n",
    "# path may contain a bunch of 0s at the start due to miscoding\n",
    "mouselab_datas[\"path\"] = mouselab_datas[\"path\"].apply(lambda path : eval(path)[-3:])\n",
    "\n",
    "node_classification = {key : [str(node) for node in val] for key, val in node_classification.items()}\n",
    "mouselab_datas = add_click_count_columns(mouselab_datas, node_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO would be nice to refactor this out and import it\n",
    "def expand_range_dictionary(input_dictionary):\n",
    "    trial_to_block = {}\n",
    "    for block, trial_range in input_dictionary.items():\n",
    "        if isinstance(trial_range, str):\n",
    "            for trial_index in eval(trial_range):\n",
    "                trial_to_block[trial_index] = block\n",
    "        else:\n",
    "            trial_to_block[block] = expand_range_dictionary(trial_range)\n",
    "    return trial_to_block\n",
    "\n",
    "if ranges_to_extract:\n",
    "    trial_to_block = expand_range_dictionary(ranges_to_extract)\n",
    "\n",
    "    mouselab_datas[\"block\"] = mouselab_datas.apply(lambda row: trial_to_block[row[\"run\"]][row[\"trial_index\"]] if row[\"run\"] in trial_to_block else trial_to_block[row[\"trial_index\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaires = full_data[\"survey\"]\n",
    "questionnaires[\"correct\"] = np.nan\n",
    "\n",
    "if \"name\" not in questionnaires:\n",
    "    questionnaires[\"name\"] = np.nan\n",
    "\n",
    "questionnaires[\"name\"] = questionnaires.apply(\n",
    "    lambda row: get_mouselab_quiz_name(row, mouselab_mapping) if not isinstance(row[\"name\"], str) else row[\"name\"],\n",
    "    axis=1)\n",
    "\n",
    "mouselab_questionnaires = questionnaires[questionnaires[\"name\"].isin(mouselab_mapping.values())].reset_index()\n",
    "questionnaires = questionnaires[~questionnaires[\"name\"].isin(mouselab_mapping.values())].reset_index()\n",
    "\n",
    "mouselab_questionnaires[\"name\"] = mouselab_questionnaires.apply(lambda row: get_mouselab_quiz_name(row, mouselab_mapping) if not isinstance(row[\"name\"], str) else row[\"name\"],axis=1)\n",
    "mouselab_questionnaires[\"responses\"] = mouselab_questionnaires.apply(\n",
    "    lambda row: str({k.split(\"_\")[1]: v for k, v in eval(row[\"responses\"]).items() if (k.split(\"_\")[1] != \"Q0\") and ((row[\"name\"] == \"mouselab-quiz-pre\" and k.split(\"_\")[0] == \"P8\") or (row[\"name\"] == \"mouselab-quiz-post\" and k.split(\"_\")[0] == \"P0\"))}), axis=1)\n",
    "mouselab_questionnaires = score_mouselab_questionnaires(mouselab_questionnaires, mouselab_quiz_solutions, mouselab_column_identifier)\n",
    "mouselab_quiz = mouselab_questionnaires.drop_duplicates([\"pid\",\"question_id\"], keep=\"last\")\n",
    "\n",
    "pivoted_mouselab_quiz = mouselab_quiz.pivot_table(values=\"score\", index=[\"pid\",\"run\"], columns=\"question_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_passers = get_quiz_passer_ids(mouselab_questionnaires, max_attempts=max_attempts, passing_score=passing_score, identifying_columns = [\"pid\", \"run\"])\n",
    "passed_all_quizzes = list(set.intersection(*map(set,quiz_passers.values())))\n",
    "print(f\"Number who passed quizzes: {len(passed_all_quizzes)}\")\n",
    "\n",
    "passed_all_quizzes = [(pid, run) for pid, run in passed_all_quizzes if pid not in straightliners]\n",
    "print(f\"Number who passed quizzes and didn't straightline: {len(passed_all_quizzes)}\")\n",
    "\n",
    "pivoted_mouselab_quiz[\"passed_quizzes\"] = 0\n",
    "pivoted_mouselab_quiz.loc[passed_all_quizzes, \"passed_quizzes\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_and_demo = pivoted_mouselab_quiz.join(demographics)\n",
    "quiz_and_demo = quiz_and_demo.merge(individual_variables, how=\"left\", on=[\"pid\", \"run\"])\n",
    "\n",
    "quiz_and_demo_subselection = quiz_and_demo[quiz_and_demo.apply(lambda row: (row[\"pid\"], row[\"run\"]) in \\\n",
    "                                 passed_all_quizzes, axis=1)].copy(deep=True)\n",
    "\n",
    "quiz_and_demo_subselection[\"gender\"]=quiz_and_demo_subselection[\"gender\"].replace(np.nan, \"participants with no demographic\")\n",
    "ages = [int(age) for age in quiz_and_demo_subselection[\"age\"] if not pd.isnull(age)]\n",
    "\n",
    "gender_values, gender_counts = np.unique(quiz_and_demo_subselection[\"gender\"].values, return_counts = True)\n",
    "print(get_demo_string(ages, gender_counts, gender_values))\n",
    "\n",
    "\n",
    "if len(questionnaires)>0:\n",
    "        scored_questionnaire_df[scored_questionnaire_df.apply(lambda row: (row[\"pid\"], row[\"run\"]) in \\\n",
    "                                 passed_all_quizzes, axis=1)].to_csv(processed_data_path.joinpath(\"questionnaires.csv\"))\n",
    "quiz_and_demo[quiz_and_demo.apply(lambda row: (row[\"pid\"], row[\"run\"]) in \\\n",
    "                                 passed_all_quizzes, axis=1)].to_csv(processed_data_path.joinpath(\"quiz-and-demo.csv\"))\n",
    "mouselab_datas[mouselab_datas.apply(lambda row: (row[\"pid\"], row[\"run\"]) in \\\n",
    "                                 passed_all_quizzes, axis=1)].to_csv(processed_data_path.joinpath(\"mouselab-mdp.csv\"))\n",
    "individual_variables[individual_variables.apply(lambda row: (row[\"pid\"], row[\"run\"]) in \\\n",
    "                                 passed_all_quizzes, axis=1)].to_csv(processed_data_path.joinpath(\"individual-variables.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_questionnaires = scored_questionnaire_df[scored_questionnaire_df.apply(lambda row: (row[\"pid\"], row[\"run\"]) in \\\n",
    "                                 passed_all_quizzes, axis=1)]\n",
    "\n",
    "individual_items = valid_questionnaires.pivot_table(\n",
    "        index=[\"pid\"], columns=\"question_id\", values=\"score\"\n",
    "    )\n",
    "\n",
    "#DOSPERT rationality index\n",
    "for dospert_item in range(1, 31):\n",
    "    individual_items[f\"dospert-rp.{dospert_item}\"] = individual_items[f\"dospert-rp.{dospert_item}\"].astype(float)\n",
    "    individual_items[f\"dospert-eb.{dospert_item}\"] = individual_items[f\"dospert-eb.{dospert_item}\"].astype(float)\n",
    "    individual_items[f\"dospert.{dospert_item}\"] = individual_items[f\"dospert.{dospert_item}\"].astype(float)\n",
    "\n",
    "for dospert_item in range(1, 31):\n",
    "    individual_items[f\"dospert-rational.{dospert_item}\"] = individual_items.apply(\n",
    "        lambda row: max(row[f\"dospert-rp.{dospert_item}\"] - row[f\"dospert-eb.{dospert_item}\"],\n",
    "                        7 - row[f\"dospert-rp.{dospert_item}\"]) if row[f\"dospert.{dospert_item}\"] > 4 else 0, axis=1)\n",
    "\n",
    "#prepare for adding combined score to combined dataframe\n",
    "combined_rational = individual_items.apply(\n",
    "    lambda row: sum([row[f\"dospert-rational.{dospert_item}\"] for dospert_item in range(1, 31)]), axis=1)\n",
    "combined_rational=combined_rational.reset_index()\n",
    "combined_rational[\"name\"] = \"dospert-rational\"\n",
    "combined_rational=combined_rational.rename(columns={0:\"score\"})\n",
    "\n",
    "individual_items.to_csv(processed_data_path.joinpath(\"individual_items.csv\"))\n",
    "\n",
    "summed_scores = valid_questionnaires.groupby([\"pid\", \"name\"]).sum()[\"score\"].reset_index()\n",
    "summed_scores = pd.concat([summed_scores, combined_rational])\n",
    "combined_scores = summed_scores.pivot_table(\n",
    "    index=[\"pid\"], columns=\"name\", values=\"score\"\n",
    ")\n",
    "combined_scores.to_csv(processed_data_path.joinpath(\"combined_scores.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_info = finished_df[finished_df.apply(lambda row: (row[\"pid\"], row[\"run\"]) in \\\n",
    "                                 passed_all_quizzes, axis=1)].median()\n",
    "print(f\"median time: {median_info['time_diff']:.2f}, median bonus: {median_info['final_bonus']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl-project",
   "language": "python",
   "name": "irl-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
